-- BuildVault: DB health snapshots + retention helpers
-- Date: 2026-02-18

create table if not exists public.db_health_snapshots (
  id bigint generated by default as identity primary key,
  captured_at timestamptz not null default now(),
  relation_schema text not null,
  relation_name text not null,
  row_estimate bigint not null default 0,
  dead_row_estimate bigint not null default 0,
  table_bytes bigint not null default 0,
  index_bytes bigint not null default 0,
  total_bytes bigint not null default 0,
  created_at timestamptz not null default now(),
  constraint db_health_snapshots_non_negative check (
    row_estimate >= 0
    and dead_row_estimate >= 0
    and table_bytes >= 0
    and index_bytes >= 0
    and total_bytes >= 0
  )
);

create index if not exists idx_db_health_snapshots_relation_captured
  on public.db_health_snapshots (relation_schema, relation_name, captured_at desc, id desc);

create index if not exists idx_db_health_snapshots_captured
  on public.db_health_snapshots (captured_at desc, id desc);

create table if not exists public.activity_log_archive (
  id uuid primary key,
  project_id uuid not null,
  action_type text not null,
  reference_id text,
  actor_user_id uuid,
  actor_name_snapshot text,
  metadata jsonb not null default '{}'::jsonb,
  created_at timestamptz not null,
  archived_at timestamptz not null default now(),
  constraint activity_log_archive_action_type_not_blank check (length(trim(action_type)) > 0)
);

create index if not exists idx_activity_log_archive_project_created
  on public.activity_log_archive (project_id, created_at desc);

create index if not exists idx_activity_log_archive_archived_at
  on public.activity_log_archive (archived_at desc);

create or replace function public.capture_db_health_snapshot()
returns integer
language plpgsql
security definer
set search_path = public, pg_catalog
as $$
declare
  inserted_count integer := 0;
begin
  insert into public.db_health_snapshots (
    captured_at,
    relation_schema,
    relation_name,
    row_estimate,
    dead_row_estimate,
    table_bytes,
    index_bytes,
    total_bytes
  )
  select
    now(),
    n.nspname,
    c.relname,
    greatest(coalesce(s.n_live_tup::bigint, c.reltuples::bigint, 0), 0),
    greatest(coalesce(s.n_dead_tup::bigint, 0), 0),
    pg_relation_size(c.oid),
    pg_indexes_size(c.oid),
    pg_total_relation_size(c.oid)
  from pg_class c
  inner join pg_namespace n on n.oid = c.relnamespace
  left join pg_stat_user_tables s on s.relid = c.oid
  where n.nspname = 'public'
    and c.relkind = 'r'
    and c.relname = any (
      array[
        'projects',
        'project_members',
        'project_phases',
        'organizations',
        'organization_members',
        'folders',
        'media',
        'notes',
        'activity_log',
        'public_project_comments',
        'public_media_posts'
      ]::text[]
    );

  get diagnostics inserted_count = row_count;
  return inserted_count;
end;
$$;

create or replace function public.archive_old_activity_log(
  p_keep_days integer default 180,
  p_batch_size integer default 10000
)
returns table (
  moved_count integer,
  remaining_count bigint,
  cutoff timestamptz
)
language plpgsql
security definer
set search_path = public, pg_catalog
as $$
declare
  v_keep_days integer := greatest(coalesce(p_keep_days, 180), 30);
  v_batch_size integer := greatest(coalesce(p_batch_size, 10000), 1);
  v_cutoff timestamptz := now() - make_interval(days => v_keep_days);
begin
  with candidates as (
    select a.id
    from public.activity_log a
    where a.created_at < v_cutoff
    order by a.created_at asc
    limit v_batch_size
  ),
  moved as (
    insert into public.activity_log_archive (
      id,
      project_id,
      action_type,
      reference_id,
      actor_user_id,
      actor_name_snapshot,
      metadata,
      created_at,
      archived_at
    )
    select
      a.id,
      a.project_id,
      a.action_type,
      a.reference_id,
      a.actor_user_id,
      a.actor_name_snapshot,
      a.metadata,
      a.created_at,
      now()
    from public.activity_log a
    inner join candidates c on c.id = a.id
    on conflict (id) do nothing
    returning id
  ),
  deleted as (
    delete from public.activity_log a
    using moved m
    where a.id = m.id
    returning 1
  )
  select count(*)::integer into moved_count from deleted;

  select count(*)::bigint
  into remaining_count
  from public.activity_log a
  where a.created_at < v_cutoff;

  cutoff := v_cutoff;
  return next;
end;
$$;

create or replace view public.db_health_latest as
with ranked as (
  select
    s.*,
    row_number() over (
      partition by s.relation_schema, s.relation_name
      order by s.captured_at desc, s.id desc
    ) as row_rank
  from public.db_health_snapshots s
),
latest as (
  select * from ranked where row_rank = 1
),
previous as (
  select * from ranked where row_rank = 2
)
select
  l.relation_schema,
  l.relation_name,
  l.captured_at,
  l.row_estimate,
  l.dead_row_estimate,
  l.table_bytes,
  l.index_bytes,
  l.total_bytes,
  coalesce(l.row_estimate - p.row_estimate, 0)::bigint as row_growth_since_last,
  coalesce(l.total_bytes - p.total_bytes, 0)::bigint as bytes_growth_since_last
from latest l
left join previous p
  on p.relation_schema = l.relation_schema
 and p.relation_name = l.relation_name;

create or replace view public.db_health_weekly_growth as
with weekly_ranked as (
  select
    s.relation_schema,
    s.relation_name,
    date_trunc('week', s.captured_at)::date as week_start,
    s.captured_at,
    s.id,
    s.row_estimate,
    s.total_bytes,
    row_number() over (
      partition by s.relation_schema, s.relation_name, date_trunc('week', s.captured_at)
      order by s.captured_at desc, s.id desc
    ) as week_rank
  from public.db_health_snapshots s
),
weekly as (
  select
    relation_schema,
    relation_name,
    week_start,
    captured_at,
    row_estimate,
    total_bytes
  from weekly_ranked
  where week_rank = 1
),
with_lag as (
  select
    w.*,
    lag(w.row_estimate) over (
      partition by w.relation_schema, w.relation_name
      order by w.week_start asc
    ) as prev_row_estimate,
    lag(w.total_bytes) over (
      partition by w.relation_schema, w.relation_name
      order by w.week_start asc
    ) as prev_total_bytes
  from weekly w
)
select
  relation_schema,
  relation_name,
  week_start,
  captured_at,
  row_estimate,
  total_bytes,
  coalesce(row_estimate - prev_row_estimate, 0)::bigint as weekly_row_growth,
  coalesce(total_bytes - prev_total_bytes, 0)::bigint as weekly_bytes_growth
from with_lag;

create or replace view public.db_health_alerts as
with latest as (
  select
    l.relation_schema,
    l.relation_name,
    l.captured_at,
    l.row_estimate,
    l.dead_row_estimate,
    l.total_bytes,
    l.row_growth_since_last,
    l.bytes_growth_since_last,
    case
      when l.row_estimate > 0
        then coalesce(l.dead_row_estimate::numeric / nullif(l.row_estimate, 0), 0)
      else 0
    end as dead_ratio
  from public.db_health_latest l
)
select
  relation_schema,
  relation_name,
  captured_at,
  row_estimate,
  total_bytes,
  row_growth_since_last,
  bytes_growth_since_last,
  round(dead_ratio, 4) as dead_ratio,
  case
    when relation_name = 'activity_log' and row_estimate >= 1000000 then 'critical'
    when relation_name = 'media' and total_bytes >= (20::bigint * 1024 * 1024 * 1024) then 'critical'
    when dead_ratio >= 0.40 then 'critical'
    when relation_name = 'activity_log' and row_estimate >= 250000 then 'warning'
    when relation_name = 'media' and total_bytes >= (5::bigint * 1024 * 1024 * 1024) then 'warning'
    when dead_ratio >= 0.20 then 'warning'
    else 'ok'
  end as severity,
  case
    when relation_name = 'activity_log' and row_estimate >= 1000000 then 'Archive old activity rows now'
    when relation_name = 'media' and total_bytes >= (20::bigint * 1024 * 1024 * 1024) then 'Move media blobs to Storage and keep metadata only'
    when dead_ratio >= 0.40 then 'Run VACUUM (ANALYZE) and review write churn'
    when relation_name = 'activity_log' and row_estimate >= 250000 then 'Start weekly archive batches'
    when relation_name = 'media' and total_bytes >= (5::bigint * 1024 * 1024 * 1024) then 'Plan Storage bucket lifecycle + compression'
    when dead_ratio >= 0.20 then 'Schedule maintenance vacuum window'
    else 'No action needed'
  end as recommendation
from latest;

alter table public.db_health_snapshots enable row level security;
alter table public.activity_log_archive enable row level security;

revoke all on table public.db_health_snapshots from anon, authenticated;
revoke all on table public.activity_log_archive from anon, authenticated;
revoke all on public.db_health_latest from anon, authenticated;
revoke all on public.db_health_weekly_growth from anon, authenticated;
revoke all on public.db_health_alerts from anon, authenticated;

grant execute on function public.capture_db_health_snapshot() to service_role;
grant execute on function public.archive_old_activity_log(integer, integer) to service_role;
grant select on public.db_health_snapshots to service_role;
grant select on public.activity_log_archive to service_role;
grant select on public.db_health_latest to service_role;
grant select on public.db_health_weekly_growth to service_role;
grant select on public.db_health_alerts to service_role;

select public.capture_db_health_snapshot();
